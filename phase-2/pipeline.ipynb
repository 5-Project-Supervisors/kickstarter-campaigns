{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://2b282a8d538e:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1590338068681)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:34:24,391 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
       "import org.apache.spark.ml.Pipeline\n",
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@6f0d304a\n",
       "df: org.apache.spark.sql.DataFrame = [ID: int, name: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import necessary libraries\n",
    "\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "// read data from csv into a dataframe\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "var df = sqlContext\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"escape\",\"\\\"\")\n",
    "    .load(\"ks-projects-201801.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discretizeNameLength: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "dropBadDates: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "formatDates: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "binaryState: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "discretizeContinousVar: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "dummyVariables: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "checkCurrencyCountry: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "pipeline: (df: org.apache.spark.sql.DataFrame, fns: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame*)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This function creates a feature for the length of the project name sorted into discrete bins\n",
    "def discretizeNameLength(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame={\n",
    "    // Get the number of characters of each name\n",
    "    var tmpdf = df.withColumn(\"namelength\", length($\"name\"))\n",
    "    // set number of bins to discretize name length\n",
    "    val bins = 5\n",
    "    // Use the built in QuantileDiscretizer to discretize the lenghts into 5 bins\n",
    "    val discretizerLength = new QuantileDiscretizer()\n",
    "      .setInputCol(\"namelength\")\n",
    "      .setOutputCol(\"namelengthBinned\")\n",
    "      .setNumBuckets(bins)\n",
    "    // Fit the discretizer to the data and transform the dataframe\n",
    "    discretizerLength.fit(tmpdf).transform(tmpdf)\n",
    "        .na.fill(-1,Array(\"namelengthBinned\")) // Any NULL will be filled with -1 to show that the name was empty\n",
    "        .drop(\"namelength\") // Remove the namelength column as this will not be required and return the transformed DataFrame\n",
    "}\n",
    "\n",
    "// This function drops invalid dates\n",
    "// Kickstarter started in 2009 and any dates before this should be dropped from the dataset\n",
    "def dropBadDates(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame = {\n",
    "    df.filter((col(\"launched\")>\"2009-01-01 00:00:00\")&&(col(\"deadline\")>\"2009-01-01 00:00:00\"))\n",
    "}\n",
    "\n",
    "// This function formats the dates into a consistant format\n",
    "// It also extracts features including the month and year from the dates as well as the project duration\n",
    "def formatDates(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame = {\n",
    "    df\n",
    "    // The specific time the project is uploaded is not relevant to the model, so only the dates are formated as \"yyyy-MM-dd\"\n",
    "    .withColumn(\"launched\", date_format(col(\"launched\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"deadline\", date_format(col(\"deadline\"), \"yyyy-MM-dd\"))\n",
    "    // Extract the month from the dates as an additional feature\n",
    "    .withColumn(\"launched_month\", date_format(col(\"launched\"), \"MM\"))\n",
    "    .withColumn(\"deadline_month\", date_format(col(\"deadline\"), \"MM\"))\n",
    "    // Extract the year from the dates as an additional feature\n",
    "    .withColumn(\"launched_year\", date_format(col(\"launched\"), \"yyyy\"))\n",
    "    .withColumn(\"deadline_year\", date_format(col(\"deadline\"), \"yyyy\"))\n",
    "    // Create a feature to show how long the project was opened for\n",
    "    .withColumn(\"Duration(Days)\",datediff(col(\"deadline\"),col(\"launched\")))\n",
    "}\n",
    "\n",
    "// This function creates a feature which converts the project state column to a binary outcome\n",
    "// It drops live and undefined projects as the outcome is undefined\n",
    "// a 1 is assigned for \"Successful\" and 0 for everything else\n",
    "def binaryState(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame={   \n",
    "    df.filter((col(\"state\")!==\"live\")&&(col(\"state\")!==\"undefined\"))\n",
    "    .withColumn(\"stateBinary\", (col(\"state\")===\"successful\")\n",
    "    .cast(IntegerType))\n",
    "}\n",
    "\n",
    "def discretizeContinousVar(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame = {\n",
    "    // The below code is modified from an example posted by user jarias \n",
    "    // on stackoverflow:\n",
    "    // https://stackoverflow.com/questions/43639252/how-to-use-spark-quantilediscretizer-on-multiple-columns\n",
    "\n",
    "    // number of discrete bins\n",
    "    val bins = 10\n",
    "\n",
    "    // define columns with DoubleType as continuous\n",
    "    val continuous = df.dtypes.filter(_._2 == \"DoubleType\").map (_._1)\n",
    "\n",
    "    // apply QuantileDiscretizer on continuous columns, output as colname_discrete\n",
    "    val discretizer = new QuantileDiscretizer()\n",
    "      .setInputCols(continuous)\n",
    "      .setOutputCols(continuous.map(x => s\"${x}_discrete\"))\n",
    "      .setNumBuckets(bins) // set bins\n",
    "\n",
    "    // apply discretizer to dataframe\n",
    "    val pipeline = new Pipeline().setStages(Array(discretizer))\n",
    "    val model = pipeline.fit(df)\n",
    "    model.transform(df)\n",
    "}\n",
    "\n",
    "// This function creates dummy variables for selected categorial columns\n",
    "def dummyVariables(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame={\n",
    "    // List the categorical columns to create dummy variables from\n",
    "    // There are too many categories and this feature is encapsulated by the main_categories so this was left out\n",
    "    val categoricalCols = Array(\"country\",\"currency\",\"main_category\")\n",
    "    var tmpdf = df\n",
    "    // Iterate over each of the categorical columns and get a set of unique values\n",
    "    for (cols<-categoricalCols){\n",
    "        val setofcat = tmpdf.select(cols).collect().map(_(0)).toSet\n",
    "        // For each of the unique values create a new dummy variable to indicate the presence of the value in each row\n",
    "        for (cat<-setofcat){\n",
    "            tmpdf = tmpdf.withColumn(cols+\"_\"+cat, (col(cols) === cat).cast(IntegerType))\n",
    "        }\n",
    "    }\n",
    "   tmpdf\n",
    "}\n",
    "\n",
    "// This function checks that the currency and country columns are valid and drops invalid rows\n",
    "def checkCurrencyCountry(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame={ \n",
    "    // valid kickstarter currencies according to: \n",
    "    // https://www.kickstarter.com/blog/new-view-kickstarter-in-your-currency\n",
    "    val currencyList = List(\"AUD\",\"GBP\",\"CAD\",\"DKK\",\"EUR\",\"HKD\",\n",
    "                            \"JPY\",\"MXN\",\"NZD\",\"NOK\",\"SGD\",\"SEK\",\n",
    "                            \"CHF\",\"USD\")\n",
    "    // valid country codes for kickstarter based on: \n",
    "    // https://help.kickstarter.com/hc/en-us/articles/115005128594-Who-can-use-Kickstarter-\n",
    "    // and\n",
    "    // https://www.realifewebdesigns.com/web-marketing/abbreviations-countries.asp\n",
    "    val validKsCountries = List(\"NL\",\"MX\",\"AT\",\"HK\",\"AU\",\"CA\",\"GB\",\n",
    "                                \"DE\",\"ES\",\"US\",\"FR\",\"CH\",\"SG\",\"IT\",\n",
    "                                \"SE\",\"JP\",\"NZ\",\"IE\",\"BE\",\"NO\",\"LU\",\n",
    "                                \"DK\") \n",
    "\n",
    "    // drops rows with invalid currencies or countries (if there are any)\n",
    "    df.filter((col(\"currency\").isin(currencyList:_*))&&(col(\"country\").isin(validKsCountries:_*)))\n",
    "}\n",
    "\n",
    "// This function applys a series of functions to a dataframe and returns the transformed dataframe\n",
    "def pipeline(df:org.apache.spark.sql.DataFrame, fns:org.apache.spark.sql.DataFrame=>org.apache.spark.sql.DataFrame*) = {\n",
    "    var tmpdf = df\n",
    "    for (fn<-fns) {\n",
    "        tmpdf = fn(tmpdf)\n",
    "    }\n",
    "    tmpdf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-24 16:36:12,237 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cleaneddf: org.apache.spark.sql.DataFrame = [ID: int, name: string ... 76 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply the pipeline fucntion to get a transformed and cleaned dataframe\n",
    "val cleaneddf = pipeline(df, discretizeContinousVar,\n",
    "                        discretizeNameLength,\n",
    "                        dropBadDates,\n",
    "                        checkCurrencyCountry,\n",
    "                        formatDates,\n",
    "                        binaryState,\n",
    "                        dummyVariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Send cleaned data to a csv file\n",
    "cleaneddf.coalesce(1).write.mode(\"overwrite\")\n",
    "                    .option(\"header\",\"true\")\n",
    "                    .option(\"quote\", \"\\\"\")\n",
    "                    .option(\"escape\",\"\\\"\")\n",
    "                    .csv(\"ks-projects-201801_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
