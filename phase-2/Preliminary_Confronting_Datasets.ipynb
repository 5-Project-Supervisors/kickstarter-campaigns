{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Confrontation\n",
    "\n",
    "Preliminary EDA and cleaning was done on the two Kickstarter datasets, to determine how to combine them, or whether to pick one dataset over another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// Create a Spark Context\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "// Note: Initial cleaning step added so that \"\" and \\ are not escaped in strings. This is because movie names with commas\n",
    "// in strings were being delimited when brought in\n",
    "\n",
    "// First dataset\n",
    "var df = sqlContext\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .load(\"ks-projects-201612.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Second dataset\n",
    "var df2 = sqlContext\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .load(\"ks-projects-201801.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get the dimensions of both years\n",
    "print(\"2016: \")\n",
    "println((df.count(), df.columns.size))\n",
    "print(\"2018: \")\n",
    "println((df2.count(), df2.columns.size))\n",
    "\n",
    "// 2016 has 323,760 rows and 17 columns\n",
    "// 2018 has 378,661 rows and 15 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Look at the first 20 rows of 2016 dataframe\n",
    "\n",
    "df.show()\n",
    "\n",
    "// Looks like it is erroneously bringing in four columns (_c13, c_14, c_15, c_16) which are just nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Look at first 20 rows of 2018 dataframe\n",
    "\n",
    "df2.show()\n",
    "// No blank columns in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Look at data types of columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes\n",
    "// Looks like the 2018 dataset has more appropriate string types. The 2016 one has strings for deadline, goal, launched\n",
    "// pledged, and usd pledged, which should be dates and numbers/doubles, as in the 2018 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We need to check 2016 and 2018 for duplicates. We will do this by merging 2016 to 2018 on the ID. We will compare IDs,\n",
    "// as well as other columns. We will then choose how we process these datasets - e.g. to union them, or if only 2018\n",
    "// dataset is required.\n",
    "\n",
    "// Change the name of all the columns in the 2016 dataset. Renaming the 2016 dataset so that we can compare both later\n",
    "// to see what columns are duplicates.\n",
    "\n",
    "// Also renaming the 'usd pledged' variable in 2018 with an underscore so it is easier to refer to in code\n",
    "\n",
    "val df_renamed = \n",
    "    df.withColumnRenamed(\"ID \", \"ID_2016\")\n",
    "    .withColumnRenamed(\"name \", \"name_2016\")\n",
    "    .withColumnRenamed(\"category \", \"category_2016\")\n",
    "    .withColumnRenamed(\"main_category \", \"main_category_2016\")\n",
    "    .withColumnRenamed(\"currency \", \"currency_2016\")\n",
    "    .withColumnRenamed(\"deadline \", \"deadline_2016\")\n",
    "    .withColumnRenamed(\"goal \", \"goal_2016\")\n",
    "    .withColumnRenamed(\"launched \", \"launched_2016\")\n",
    "    .withColumnRenamed(\"pledged \", \"pledged_2016\")\n",
    "    .withColumnRenamed(\"state \", \"state_2016\")\n",
    "    .withColumnRenamed(\"backers \", \"backers_2016\")\n",
    "    .withColumnRenamed(\"usd pledged \", \"usd_pledged_2016\")\n",
    "\n",
    "val df2_renamed = df2.withColumnRenamed(\"usd pledged\", \"usd_pledged\")\n",
    "\n",
    "// Remove the original dataframes to save space.\n",
    "df.unpersist()\n",
    "df2.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Before we do a merge, we need to check if the ID values in both datasets are unique.\n",
    "// We'll do a group by and count the IDs in both datasets. If there are duplicates, they will show up on this list as having\n",
    "// appeared two or more times.\n",
    "\n",
    "df_renamed.groupBy(\"ID_2016\").count()\n",
    "  .filter($\"count\" >= 2)\n",
    "  .show()\n",
    "\n",
    "// Result: The 2016 dataframe has no duplicate IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_renamed.groupBy(\"ID\").count()\n",
    "  .filter($\"count\" >= 2)\n",
    "  .show()\n",
    "\n",
    "// Result: The 2018 dataframe has no duplicate IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Since IDs are unique, we will do a left join to see how many 2018 IDs are in the 2016 file. \n",
    "// If there are no nulls in the join, then the 2018 file contains all IDs from the 2016 file\n",
    "\n",
    "val joined_df = df_renamed.join(df2_renamed, $\"ID_2016\" === $\"ID\", \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Count the number of rows in joined_df. Should have the same number of rows as the 2016 dataset, as it's a left join.\n",
    "joined_df.count()\n",
    "//Result: 323,750 is the count of the 2016 dataset, so the join was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now check for 'ID' values that are NULL (i.e. didn't match)\n",
    "\n",
    "joined_df.filter(\"ID is null\").show\n",
    "// Result: Returns an empty table. There are no nulls, so all IDs in the 2016 dataset are in 2018 also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// As a further check, check how many rows have all 2016 columns equal the 2018 columns. The 2016 columns are named with a\n",
    "// '_2016' suffix. Do a count.\n",
    "\n",
    "joined_df.filter(\"ID_2016 = ID and name_2016 = name and category_2016 = category and main_category_2016 = main_category and currency_2016 = currency and deadline_2016 = deadline and goal_2016 = goal and launched_2016 = launched and pledged_2016 = pledged and state_2016 = state and goal_2016 = goal and backers_2016 = backers and usd_pledged_2016 = usd_pledged\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// There are relatively few records where all columns match.\n",
    "// This may be a parsing issue due to the different data types between 2016 and 2018 columns.\n",
    "\n",
    "// Instead, we'll check individual fields to get a taste of how name, category, main_category, currency, \n",
    "// and pledged/goal/backers don't match for the same ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and name_2016 = name\").count()\n",
    "// Most names match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// What name records didn't match? Take a look at the top 20\n",
    "joined_df.filter(\"ID_2016 = ID and name_2016 != name\").select(\"ID\", \"name_2016\", \"name\").show()\n",
    "// Result: The names are the same. There are likely small differences in text being picked up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and category_2016 = category\").count()\n",
    "// Most category records match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and main_category_2016 = main_category\").count()\n",
    "// Most main_category records match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and currency_2016 = currency\").count()\n",
    "// Most currency records match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and goal_2016 = goal\").count()\n",
    "// Most goal records match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and backers_2016 = backers\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and backers_2016 != backers\").select(\"ID\", \"backers_2016\", \"state_2016\", \"backers\").show()\n",
    "// The backers numbers look to have legitimate differences. However, all the numbers in the sample \n",
    "// come from records that were 'live' in 2016 (i.e. ongoing kickstarters), so the 2018 data would \n",
    "// represent legitimate, updated backer information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Confirming all instances of backers in 2018 being greater than 2016 are due to the 2016 campaigns being live at the time:\n",
    "joined_df.filter(\"ID_2016 = ID and backers_2016 < backers\").groupBy(\"state_2016\").count()\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and usd_pledged_2016 = usd_pledged\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.filter(\"ID_2016 = ID and usd_pledged_2016 != usd_pledged\").select(\"ID\", \"usd_pledged_2016\", \"usd_pledged\").show()\n",
    "// 2018 figures are rounded to the nearest cent which causes the difference. Nearest cent is preferable given this is\n",
    "// dollar values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the 2018 dataset includes all values from the 2016 dataset. Justifications: \n",
    "    1) It has all the IDs that the 2016 dataset has. \n",
    "    2) All the columns are assigned the correct data type when imported in 2018, but not for 2016. \n",
    "    3) 2018 has the same column values as 2016 for those records that are shared\n",
    "    4) It has updated 'backer' counts for those campaigns that were 'live' in 2016, \n",
    "    5) It has correctly assigned decimals, rounded to two places, for the usd_pledged variable\n",
    "\n",
    "Therefore, we will use the 2018 dataset exclusively as an input into the cleaning pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
