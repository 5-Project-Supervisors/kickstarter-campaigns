{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/06/08 12:03:50 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "\n",
      "\n",
      "Deleted /tmp/kickstarter\n",
      "\n",
      "\n",
      "-rwxr-xr-x   1 root root   58030359 2020-05-29 11:04 /tmp/kickstarter/ks-projects-201801.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm    -r  /tmp/kickstarter*\n",
    "! hadoop fs -mkdir -p  /tmp/kickstarter\n",
    "! hadoop fs -put   -p  ks-projects-201801.csv     /tmp/kickstarter/\n",
    "! hadoop fs -ls        /tmp/kickstarter/ks-projects-201801.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://8139f9c065d1:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591617837926)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator}\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "df: org.apache.spark.sql.DataFrame = [ID: int, name: string ... 13 more fields]\n",
       "res0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: int, name: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import necessary libraries\n",
    "\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "// Set log level to ERROR (less verbose)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "// read data from HDFS into a spark dataframe\n",
    "var df = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"escape\",\"\\\"\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/kickstarter/ks-projects-201801.csv\")\n",
    "\n",
    "//caching for execution speed\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discretizeNameLength: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "dropBadDates: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "formatDates: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "binaryState: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "discretizeContinousVar: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "checkCurrencyCountry: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "cleaningpipeline: (df: org.apache.spark.sql.DataFrame, fns: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame*)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This function creates a feature for the length of the project name sorted into discrete bins\n",
    "def discretizeNameLength(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame={\n",
    "    // Get the number of characters of each name\n",
    "    var tmpdf = df.withColumn(\"namelength\", length($\"name\"))\n",
    "    // set number of bins to discretize name length\n",
    "    val bins = 5\n",
    "    // Use the built in QuantileDiscretizer to discretize the lenghts into 5 bins\n",
    "    val discretizerLength = new QuantileDiscretizer()\n",
    "      .setInputCol(\"namelength\")\n",
    "      .setOutputCol(\"namelengthBinned\")\n",
    "      .setNumBuckets(bins)\n",
    "    // Fit the discretizer to the data and transform the dataframe\n",
    "    discretizerLength.fit(tmpdf).transform(tmpdf)\n",
    "        .na.fill(0,Array(\"namelengthBinned\")) // Any NULL will be filled with 0\n",
    "        .drop(\"namelength\") // Remove the namelength column as this will not be required and return the transformed DataFrame\n",
    "}\n",
    "\n",
    "// This function drops invalid dates\n",
    "// Kickstarter started in 2009 and any dates before this should be dropped from the dataset\n",
    "def dropBadDates(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame = {\n",
    "    df.filter((col(\"launched\")>\"2009-01-01 00:00:00\")&&(col(\"deadline\")>\"2009-01-01 00:00:00\"))\n",
    "}\n",
    "\n",
    "// This function formats the dates into a consistant format\n",
    "// It also extracts features including the month and year from the dates as well as the project duration\n",
    "def formatDates(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame = {\n",
    "    df\n",
    "    // The specific time the project is uploaded is not relevant to the model, so only the dates are formated as \"yyyy-MM-dd\"\n",
    "    .withColumn(\"launched\", date_format(col(\"launched\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"deadline\", date_format(col(\"deadline\"), \"yyyy-MM-dd\"))\n",
    "    // Extract the month from the dates as an additional feature\n",
    "    .withColumn(\"launched_month\", date_format(col(\"launched\"), \"MM\").cast(IntegerType))\n",
    "    .withColumn(\"deadline_month\", date_format(col(\"deadline\"), \"MM\").cast(IntegerType))\n",
    "    // Extract the year from the dates as an additional feature\n",
    "    .withColumn(\"launched_year\", date_format(col(\"launched\"), \"yyyy\").cast(IntegerType))\n",
    "    .withColumn(\"deadline_year\", date_format(col(\"deadline\"), \"yyyy\").cast(IntegerType))\n",
    "    // Create a feature to show how long the project was opened for\n",
    "    .withColumn(\"Duration(Days)\",datediff(col(\"deadline\"),col(\"launched\")).cast(DoubleType))\n",
    "}\n",
    "\n",
    "// This function creates a feature which converts the project state column to a binary outcome\n",
    "// It drops live and undefined projects as the outcome is undefined\n",
    "// a 1 is assigned for \"Successful\" and 0 for everything else\n",
    "def binaryState(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame={   \n",
    "    df.filter((col(\"state\")===\"successful\")||(col(\"state\")===\"failed\"))\n",
    "    .withColumn(\"stateBinary\", when($\"state\" === \"successful\", 1).otherwise(0))  \n",
    "}\n",
    "\n",
    "\n",
    "def discretizeContinousVar(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame = {\n",
    "    // The below code is modified from an example posted by user jarias \n",
    "    // on stackoverflow:\n",
    "    // https://stackoverflow.com/questions/43639252/how-to-use-spark-quantilediscretizer-on-multiple-columns\n",
    "\n",
    "    // number of discrete bins\n",
    "    val bins = 100\n",
    "\n",
    "    // define columns with DoubleType as continuous\n",
    "    val continuous = df.dtypes.filter(_._2 == \"DoubleType\").map (_._1)\n",
    "\n",
    "    // apply QuantileDiscretizer on continuous columns, output as colname_discrete\n",
    "    val discretizer = new QuantileDiscretizer()\n",
    "      .setInputCols(continuous)\n",
    "      .setOutputCols(continuous.map(x => s\"${x}_discrete\"))\n",
    "      .setNumBuckets(bins) // set bins\n",
    "\n",
    "    // apply discretizer to dataframe\n",
    "    val pipeline = new Pipeline().setStages(Array(discretizer))\n",
    "    val model = pipeline.fit(df)\n",
    "    model.transform(df)\n",
    "}\n",
    "\n",
    "\n",
    "// This function checks that the currency and country columns are valid and drops invalid rows\n",
    "def checkCurrencyCountry(df:org.apache.spark.sql.DataFrame):org.apache.spark.sql.DataFrame={ \n",
    "    // valid kickstarter currencies according to: \n",
    "    // https://www.kickstarter.com/blog/new-view-kickstarter-in-your-currency\n",
    "    val currencyList = List(\"AUD\",\"GBP\",\"CAD\",\"DKK\",\"EUR\",\"HKD\",\n",
    "                            \"JPY\",\"MXN\",\"NZD\",\"NOK\",\"SGD\",\"SEK\",\n",
    "                            \"CHF\",\"USD\")\n",
    "    // valid country codes for kickstarter based on: \n",
    "    // https://help.kickstarter.com/hc/en-us/articles/115005128594-Who-can-use-Kickstarter-\n",
    "    // and\n",
    "    // https://www.realifewebdesigns.com/web-marketing/abbreviations-countries.asp\n",
    "    val validKsCountries = List(\"NL\",\"MX\",\"AT\",\"HK\",\"AU\",\"CA\",\"GB\",\n",
    "                                \"DE\",\"ES\",\"US\",\"FR\",\"CH\",\"SG\",\"IT\",\n",
    "                                \"SE\",\"JP\",\"NZ\",\"IE\",\"BE\",\"NO\",\"LU\",\n",
    "                                \"DK\") \n",
    "\n",
    "    // drops rows with invalid currencies or countries (if there are any)\n",
    "    df.filter((col(\"currency\").isin(currencyList:_*))&&(col(\"country\").isin(validKsCountries:_*)))\n",
    "}\n",
    "\n",
    "// This function applys a series of functions to a dataframe and returns the transformed dataframe\n",
    "def cleaningpipeline(df:org.apache.spark.sql.DataFrame, fns:org.apache.spark.sql.DataFrame=>org.apache.spark.sql.DataFrame*) = {\n",
    "    var tmpdf = df\n",
    "    for (fn<-fns) {\n",
    "        tmpdf = fn(tmpdf)\n",
    "    }\n",
    "    tmpdf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaneddf: org.apache.spark.sql.DataFrame = [ID: int, name: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply the pipeline fucntion to get a transformed and cleaned dataframe\n",
    "var cleaneddf = cleaningpipeline(df,\n",
    "                        dropBadDates,\n",
    "                        formatDates,\n",
    "                        discretizeContinousVar,\n",
    "                        checkCurrencyCountry,\n",
    "                        discretizeNameLength,\n",
    "                        binaryState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfSelected: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, main_category: string ... 10 more fields]\n",
       "featureCols: Array[String] = Array(category, main_category, currency, country, launched_month, deadline_month, launched_year, deadline_year, Duration(Days)_discrete, usd_goal_real_discrete, namelengthBinned)\n",
       "indexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_02fc6f34599a, strIdx_545d7e3677e4, strIdx_67311c2c531a, strIdx_1946a039d494, strIdx_0cc5f3ce2c80, strIdx_16865631b86c, strIdx_d94343450b2e, strIdx_b3f9908ff06b, strIdx_748a92592383, strIdx_f6b5e4daad61, strIdx_8701264b898b)\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_991b1e5481b8\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_77..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop features which will not be used by the ML model and rename the \"stateBinary\" column as the label\n",
    "var dfSelected = cleaneddf.drop(\"ID\", \"name\", \"deadline\",\"Duration(Days)\",\n",
    "                          \"goal\", \"launched\", \"pledged\", \"state\", \"backers\", \"usd pledged\", \n",
    "                          \"usd_pledged_real\", \"usd_goal_real\", \"usd pledged_discrete\", \"goal_discrete\", \n",
    "                          \"pledged_discrete\",\"usd_pledged_real_discrete\")\n",
    "                          .withColumnRenamed(\"stateBinary\",\"label\").cache() //caching for execution speed\n",
    "\n",
    "// Create an array of column names which will be combined as features\n",
    "var featureCols = dfSelected.drop(\"label\").columns\n",
    "\n",
    "// For each of the feature columns create a string indexer to transform them into a form that can be one-hot-encoded\n",
    "val indexers = featureCols.map(name=>new StringIndexer()\n",
    "    .setInputCol(name)\n",
    "    .setOutputCol(name+\"_indexed\")\n",
    "    .setHandleInvalid(\"keep\"))\n",
    "\n",
    "// One hot encode all of the feature columns\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "    .setInputCols(featureCols.map(_+\"_indexed\"))\n",
    "    .setOutputCols(featureCols.map(_+\"_encoded\"))\n",
    "\n",
    "// Make an assembler which takes the encoded output from the encoder and assembles them as vector\n",
    "val assembler = (new VectorAssembler().setInputCols(encoder.getOutputCols).setOutputCol(\"features\"))\n",
    "\n",
    "// Split the original data into training and test sets using a 80/20% splt\n",
    "val Array(training, test) = dfSelected.randomSplit(Array(0.8, 0.2),seed=42)\n",
    "\n",
    "//caching for execution speed\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "// Create Logistic Regression model\n",
    "// Parameters below were identified as optimal during the tuning phase of the project\n",
    "val lr = new LogisticRegression().\n",
    "        setFitIntercept(true).\n",
    "        setMaxIter(100).\n",
    "        setElasticNetParam(0).\n",
    "        setTol(0).\n",
    "        setStandardization(true).\n",
    "        setRegParam(0.01).\n",
    "        setThreshold(0.6).\n",
    "        setElasticNetParam(0).\n",
    "        setLabelCol(\"label\").\n",
    "        setFeaturesCol(\"features\")\n",
    "\n",
    "// Put everything into the pipeline\n",
    "// This pipeline will take in the training dataset, index and encode features and add\n",
    "// then assemble the required variables into the 'features' column, then run the logistic\n",
    "// regression on it.\n",
    "val pipeline = new Pipeline().setStages(indexers++\n",
    "                                        Array(encoder,\n",
    "                                             assembler,\n",
    "                                             lr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_66d9e7bc5226\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Fit the pipleine to the training set \n",
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "35678.0  17886.0  \n",
      "3832.0   8816.0   \n",
      "\n",
      "Summary Statistics:\n",
      "Accuracy = 0.6719929922068507\n",
      "Precision(0.0) = 0.9030118957226019\n",
      "Precision(1.0) = 0.3301625346415999\n",
      "Recall(0.0) = 0.6660816966619372\n",
      "Recall(1.0) = 0.6970271979759646\n",
      "FPR(0.0) = 0.3029728020240354\n",
      "FPR(1.0) = 0.3339183033380629\n",
      "F1-Score(0.0) = 0.7666587876313472\n",
      "F1-Score(1.0) = 0.4480813214739517\n",
      "Weighted precision: 0.793584620924189\n",
      "Weighted recall: 0.6719929922068507\n",
      "Weighted F1 score: 0.705803235889084\n",
      "Weighted false positive rate: 0.308884097568949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "results: org.apache.spark.sql.DataFrame = [category: string, main_category: string ... 36 more fields]\n",
       "evaluation_results_only: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[314] at rdd at <console>:51\n",
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@3716b2dd\n",
       "accuracy: Double = 0.6719929922068507\n",
       "labels: Array[Double] = Array(0.0, 1.0)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform the test data using the model to predict whether a project will be successful or not\n",
    "val results = model.transform(test)\n",
    "\n",
    "// Put the actual and predicted in a RDD for analysis\n",
    "val evaluation_results_only = results\n",
    "                                    .select(\"label\", \"prediction\")\n",
    "                                    .as[(Double, Double)]\n",
    "                                    .rdd.cache()\n",
    "\n",
    "// Get the metrics of how the model performed on the test data\n",
    "val metrics = new MulticlassMetrics(evaluation_results_only)\n",
    "\n",
    "// Print confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)\n",
    "\n",
    "// Print Overall Statistics\n",
    "val accuracy = metrics.accuracy\n",
    "println(\"\\nSummary Statistics:\")\n",
    "println(s\"Accuracy = $accuracy\")\n",
    "\n",
    "// Precision by label\n",
    "val labels = metrics.labels\n",
    "labels.foreach { l =>\n",
    "println(s\"Precision($l) = \" + metrics.precision(l))\n",
    "}\n",
    "\n",
    "// Recall by label\n",
    "labels.foreach { l =>\n",
    "println(s\"Recall($l) = \" + metrics.recall(l))\n",
    "}\n",
    "\n",
    "// False positive rate by label\n",
    "labels.foreach { l =>\n",
    "println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n",
    "}\n",
    "\n",
    "// F-measure by label\n",
    "labels.foreach { l =>\n",
    "println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n",
    "}\n",
    "\n",
    "// Weighted stats\n",
    "println(s\"Weighted precision: ${metrics.weightedPrecision}\")\n",
    "println(s\"Weighted recall: ${metrics.weightedRecall}\")\n",
    "println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n",
    "println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
